
The following have been reloaded with a version change:
  1) all/CUDA/12.1.0 => all/CUDA/11.7.0

wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: williambittner (william_masterthesis). Use `wandb login --relogin` to force relogin
wandb: WARNING Path /work/williamb/gnode_wandb/wandb/ wasn't writable, using system temp directory.
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /tmp/wandb/run-20241213_140717-duxa8wlg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run restful-snowball-8
wandb: ‚≠êÔ∏è View project at https://wandb.ai/william_masterthesis/PointTransformerNFrames_tmp
wandb: üöÄ View run at https://wandb.ai/william_masterthesis/PointTransformerNFrames_tmp/runs/duxa8wlg
Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]
Traceback (most recent call last):
  File "/users/williamb/dev/gnode/main.py", line 207, in <module>
    main()
  File "/users/williamb/dev/gnode/main.py", line 165, in main
    trainer.train(train_dataloader, epochs=epochs)
  File "/athenahomes/williamb/dev/gnode/trainer.py", line 59, in train
    avg_loss = self.train_one_epoch(dataloader)
  File "/athenahomes/williamb/dev/gnode/trainer.py", line 43, in train_one_epoch
    pred = self.model(X_t)
  File "/users/williamb/miniconda3/envs/gaussian_splatting_athena/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/williamb/miniconda3/envs/gaussian_splatting_athena/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/athenahomes/williamb/dev/gnode/model.py", line 152, in forward
    x = self.sequence_processor(x)  # Shape remains (B*N, S, hidden_dim)
  File "/users/williamb/miniconda3/envs/gaussian_splatting_athena/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/williamb/miniconda3/envs/gaussian_splatting_athena/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/williamb/miniconda3/envs/gaussian_splatting_athena/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 416, in forward
    output = mod(output, src_mask=mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask_for_layers)
  File "/users/williamb/miniconda3/envs/gaussian_splatting_athena/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/williamb/miniconda3/envs/gaussian_splatting_athena/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/williamb/miniconda3/envs/gaussian_splatting_athena/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 749, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal))
  File "/users/williamb/miniconda3/envs/gaussian_splatting_athena/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 757, in _sa_block
    x = self.self_attn(x, x, x,
  File "/users/williamb/miniconda3/envs/gaussian_splatting_athena/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/williamb/miniconda3/envs/gaussian_splatting_athena/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/williamb/miniconda3/envs/gaussian_splatting_athena/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1275, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/users/williamb/miniconda3/envs/gaussian_splatting_athena/lib/python3.8/site-packages/torch/nn/functional.py", line 5560, in multi_head_attention_forward
    attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
RuntimeError: CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Traceback (most recent call last):
  File "/users/williamb/dev/gnode/main.py", line 207, in <module>
    main()
  File "/users/williamb/dev/gnode/main.py", line 165, in main
    trainer.train(train_dataloader, epochs=epochs)
  File "/athenahomes/williamb/dev/gnode/trainer.py", line 59, in train
    avg_loss = self.train_one_epoch(dataloader)
  File "/athenahomes/williamb/dev/gnode/trainer.py", line 43, in train_one_epoch
    pred = self.model(X_t)
  File "/users/williamb/miniconda3/envs/gaussian_splatting_athena/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/williamb/miniconda3/envs/gaussian_splatting_athena/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/athenahomes/williamb/dev/gnode/model.py", line 152, in forward
    x = self.sequence_processor(x)  # Shape remains (B*N, S, hidden_dim)
  File "/users/williamb/miniconda3/envs/gaussian_splatting_athena/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/williamb/miniconda3/envs/gaussian_splatting_athena/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/williamb/miniconda3/envs/gaussian_splatting_athena/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 416, in forward
    output = mod(output, src_mask=mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask_for_layers)
  File "/users/williamb/miniconda3/envs/gaussian_splatting_athena/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/williamb/miniconda3/envs/gaussian_splatting_athena/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/williamb/miniconda3/envs/gaussian_splatting_athena/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 749, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal))
  File "/users/williamb/miniconda3/envs/gaussian_splatting_athena/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 757, in _sa_block
    x = self.self_attn(x, x, x,
  File "/users/williamb/miniconda3/envs/gaussian_splatting_athena/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/williamb/miniconda3/envs/gaussian_splatting_athena/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/williamb/miniconda3/envs/gaussian_splatting_athena/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1275, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/users/williamb/miniconda3/envs/gaussian_splatting_athena/lib/python3.8/site-packages/torch/nn/functional.py", line 5560, in multi_head_attention_forward
    attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
RuntimeError: CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

